{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install einops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the images\n",
    "image_folder_base = '/training_data/'\n",
    "\n",
    "folders = os.listdir(image_folder_base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, num_classes, dim, num_heads, num_layers):\n",
    "        super(ViT, self).__init__()\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_embed = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers\n",
    "        )\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, dim))\n",
    "\n",
    "        self.fc = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embedding\n",
    "\n",
    "        x = self.transformer(x,x)\n",
    "\n",
    "        latent_representation = x[:, 0]  # The first token represents the aggregated information\n",
    "\n",
    "        output = self.fc(latent_representation)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "image_size = 224\n",
    "patch_size = 16\n",
    "num_classes = 10\n",
    "dim = 256\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "\n",
    "# Instantiate the ViT model\n",
    "vit_model = ViT(image_size, patch_size, num_classes, dim, num_heads, num_layers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each image file\n",
    "for folder_name in folders:\n",
    "    image_folder = os.path.join(image_folder_base, folder_name)\n",
    "\n",
    "    image_filenames = Path(image_folder).glob('*.png')\n",
    "    for image_filename in image_filenames:\n",
    "        image_path = os.path.join(image_folder_base, image_filename)\n",
    "\n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        preprocess = transforms.Compose([transforms.Resize((image_size, image_size)),transforms.ToTensor()])\n",
    "        example_image = preprocess(image).unsqueeze(0)\n",
    "\n",
    "        # Get the latent space representation\n",
    "        latent_representation = vit_model(example_image)\n",
    "        \n",
    "        print(latent_representation.shape)\n",
    "        print(latent_representation) #Print the shape of the latent space representation\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
