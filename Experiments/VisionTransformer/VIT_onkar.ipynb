{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Mst5L4rOzn1r",
        "outputId": "f106a5c8-ebb2-4e92-ad08-f02d4423c3ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einop in /usr/local/lib/python3.10/dist-packages (0.0.1)\n",
            "Requirement already satisfied: einops>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from einop) (0.6.1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install einop\n",
        "from torch import nn\n",
        "from torch import Tensor\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, ToTensor\n",
        "from einops import rearrange, reduce, repeat\n",
        "from einops.layers.torch import Rearrange, Reduce\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "there is two method to download the data in the colab from kaggle from the link and from the name"
      ],
      "metadata": {
        "id": "aYzjoNpD-46w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets\n",
        "# !pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "i_3DiiVC881u",
        "outputId": "d23ecc4d-1b23-40c7-befa-1a4542324ce4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.10/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from opendatasets) (4.65.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (from opendatasets) (1.5.15)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from opendatasets) (8.1.4)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2023.5.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (2.27.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (1.26.16)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle->opendatasets) (6.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle->opendatasets) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/preatcher/standard-ocr-dataset\" , force = True )\n"
      ],
      "metadata": {
        "id": "wX8wnAHd-Hik",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "92645d32-43c2-481f-e62b-c213a6935647"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading standard-ocr-dataset.zip to ./standard-ocr-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 46.2M/46.2M [00:00<00:00, 258MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip data.zip\n"
      ],
      "metadata": {
        "id": "7xBwj8kFh_Gh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "data = os.listdir(\"/content/standard-ocr-dataset\")\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-DRCrDm__cOf",
        "outputId": "5199f655-96f7-4cc8-bde3-8ed41bb124ee"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['data', 'data2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_paths = []\n",
        "for i in range(0,len(data)):\n",
        "  data_path = os.path.join(\"standard-ocr-dataset\" , data[i])\n",
        "  data_paths.append(data_path)\n",
        "# print (data_paths)"
      ],
      "metadata": {
        "id": "MuohkcTZNtV_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = os.listdir('standard-ocr-dataset/data')\n",
        "#data1 = os.listdir('standard-ocr-dataset/data')\n",
        "# print(data2)\n",
        "#print(data1)"
      ],
      "metadata": {
        "id": "hGMLktGyKHMX"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am takng data2 for my training\n"
      ],
      "metadata": {
        "id": "GH958EEHQcFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_paths = []\n",
        "for i in range(0,len(data)):\n",
        "  dataset_path = os.path.join(\"standard-ocr-dataset/data2\" , data2[i])\n",
        "  dataset_paths.append(dataset_path)\n",
        "# print (dataset_paths)"
      ],
      "metadata": {
        "id": "Mt7EdHCtKiIW"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from re import I\n",
        "\n",
        "def data_tensor(datasetpath ):\n",
        "  Dataset = []\n",
        "  lables_dict = {}\n",
        "  # print(lables_dict)\n",
        "  dataset = os.listdir(datasetpath)\n",
        "  data_f = sorted(dataset)\n",
        "  for i in range(len(data_f)):\n",
        "    lables_dict[data_f[i]] = i\n",
        "    data_f_path = os.path.join(datasetpath, data_f[i])\n",
        "    data_img = os.listdir(data_f_path)\n",
        "    for j in range(len(data_img)):\n",
        "      data_img_path = os.path.join(data_f_path, data_img[j])\n",
        "      img = Image.open(data_img_path)\n",
        "      transform = Compose([Resize((64, 64)), ToTensor()])\n",
        "      im = transform(img)\n",
        "      rgb_img= im.unsqueeze(0) # add batch dim\n",
        "      rgb_img = im.expand(1,3,64,64)\n",
        "      Dataset.append([rgb_img, np.eye(36)[i]])\n",
        "\n",
        "  return Dataset\n",
        "# DAta ='/content/standard-ocr-dataset/data/testing_data'\n",
        "# testing_data= data_tensor(DAta)\n",
        "# # print(Data)\n",
        "# print(type(Data))"
      ],
      "metadata": {
        "id": "Whdws69rpC-g"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DAta ='/content/standard-ocr-dataset/data/testing_data'\n",
        "testing_data= data_tensor(DAta)"
      ],
      "metadata": {
        "id": "4KzV5stGs1mZ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in testing_dataset:\n",
        "  # print(i.shape)\n"
      ],
      "metadata": {
        "id": "b_RvvoUUS3h0"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Training_dataset_path = 'standard-ocr-dataset/data/training_data'\n",
        "training_data = data_tensor(Training_dataset_path)\n",
        "# for i in training_dataset:\n",
        "\n",
        "  # print(i.shape)"
      ],
      "metadata": {
        "id": "si706ZRjQ4wW"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class_names = sorted(os.listdir(Training_dataset_path))\n",
        "# print(class_names)\n"
      ],
      "metadata": {
        "id": "__cdj3bdVgTr"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3nq6IgzDUQJ2"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_list = []\n",
        "# print(type(Data))\n",
        "n = len(training_data)\n",
        "for i in range(n):\n",
        "  patch_size = 8 # 16 pixel\n",
        "  # print(type(training_data[i][0]))\n",
        "  pathes = rearrange(training_data[i][0], 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=patch_size,\n",
        "                   s2=patch_size)\n",
        "  patch_list.append(pathes)\n",
        "  # print(patch_list[i])\n",
        "\n",
        "# del n\n"
      ],
      "metadata": {
        "id": "FQ5eTX-U3k4d"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels: int = 3, patch_size: int = 8, emb_size: int = 64, img_size: int = 64):\n",
        "        self.patch_size = patch_size\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            # using a conv layer instead of a linear one -> performance gains\n",
        "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
        "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
        "        )\n",
        "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
        "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        b, _, _, _ = x.shape\n",
        "        x = self.projection(x)\n",
        "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)\n",
        "        # prepend the cls token to the input\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "        # add position embedding\n",
        "        x += self.positions\n",
        "        return x\n",
        "for i in range(int(len(training_data))):\n",
        "  PatchEmbedding()(training_data[i][0])"
      ],
      "metadata": {
        "id": "nDxY2GwGm85b"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size: int = 64, num_heads: int = 8, dropout: float = 0):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        # fuse the queries, keys and values in one matrix\n",
        "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
        "        self.att_drop = nn.Dropout(dropout)\n",
        "        self.projection = nn.Linear(emb_size, emb_size)\n",
        "\n",
        "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
        "        # split keys, queries and values in num_heads\n",
        "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
        "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
        "        # sum up over the last axis\n",
        "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
        "        if mask is not None:\n",
        "            fill_value = torch.finfo(torch.float32).min\n",
        "            energy.mask_fill(~mask, fill_value)\n",
        "\n",
        "        scaling = self.emb_size ** (1/2)\n",
        "        att = F.softmax(energy, dim=-1) / scaling\n",
        "        att = self.att_drop(att)\n",
        "        # sum up over the third axis\n",
        "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)\n",
        "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
        "        out = self.projection(out)\n",
        "        return out\n",
        "\n",
        "for i in range(int(len(training_data))):\n",
        "  patches_embedded = PatchEmbedding()(training_data[i][0])\n",
        "  MultiHeadAttention()(patches_embedded)"
      ],
      "metadata": {
        "id": "X-QwN8Gwnehp"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualAdd(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        res = x\n",
        "        x = self.fn(x, **kwargs)\n",
        "        x += res\n",
        "        return x"
      ],
      "metadata": {
        "id": "jVopQ03onjLD"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kQiYG8Ghxzn2"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Sequential):\n",
        "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
        "        super().__init__(\n",
        "            nn.Linear(emb_size, expansion * emb_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(drop_p),\n",
        "            nn.Linear(expansion * emb_size, emb_size),\n",
        "        )"
      ],
      "metadata": {
        "id": "367l-atlnkbM"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                 emb_size: int = 64,\n",
        "                 drop_p: float = 0.,\n",
        "                 forward_expansion: int = 4,\n",
        "                 forward_drop_p: float = 0.,\n",
        "                 ** kwargs):\n",
        "        super().__init__(\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                MultiHeadAttention(emb_size, **kwargs),\n",
        "                nn.Dropout(drop_p)\n",
        "            )),\n",
        "            ResidualAdd(nn.Sequential(\n",
        "                nn.LayerNorm(emb_size),\n",
        "                FeedForwardBlock(\n",
        "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
        "                nn.Dropout(drop_p)\n",
        "            )\n",
        "            ))"
      ],
      "metadata": {
        "id": "z-33T2I6nvIs"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(int(len(training_data))):\n",
        "  patches_embedded = PatchEmbedding()(training_data[i][0])\n",
        "  TransformerEncoderBlock()(patches_embedded).shape"
      ],
      "metadata": {
        "id": "xLE87Kp2nzvc"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XBiEZrVOZaYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Sequential):\n",
        "    def __init__(self, depth: int = 12, **kwargs):\n",
        "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])\n"
      ],
      "metadata": {
        "id": "WAjsU4Vdn6qs"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationHead(nn.Sequential):\n",
        "    def __init__(self, emb_size: int = 128, n_classes: int = 1000):\n",
        "        super().__init__(\n",
        "            Reduce('b n e -> b e', reduction='mean'),\n",
        "            nn.LayerNorm(emb_size),\n",
        "            nn.Linear(emb_size, n_classes))"
      ],
      "metadata": {
        "id": "0ih6R4X9n6b_"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "23kuDiUUrHnu"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q9SnUTjDrw7v"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ViT(nn.Sequential):\n",
        "    def __init__(self,\n",
        "                in_channels: int = 3,\n",
        "                patch_size: int = 16,\n",
        "                emb_size: int = 64,\n",
        "                img_size: int = 64,\n",
        "                depth: int = 12,\n",
        "                n_classes: int = 36,\n",
        "                **kwargs):\n",
        "        super().__init__(\n",
        "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
        "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
        "            ClassificationHead(emb_size, n_classes)\n",
        "        )\n",
        "    def save(self, path=\"best_model.pth\"):\n",
        "        torch.save(self.state_dict(), path)\n",
        "\n",
        "    def load(self, path=\"best_model.pth\"):\n",
        "        self.load_state_dict(torch.load(path))\n",
        "        self.eval()\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        train_loader,\n",
        "        optimizer=Adam,\n",
        "        loss_function=CrossEntropyLoss(),\n",
        "        learning_rate=0.001,\n",
        "        epochs=5,\n",
        "        tqdm_show=False,\n",
        "    ):\n",
        "        min_loss = float(\"inf\")\n",
        "        self.optimizer = optimizer(self.parameters(), lr=learning_rate)\n",
        "        self.loss_func = loss_function\n",
        "        self.epochs = epochs\n",
        "\n",
        "        if tqdm_show:\n",
        "            for epoch in tqdm(range(self.epochs)):\n",
        "                for batch in tqdm(train_loader):\n",
        "                    train_X, train_y = batch\n",
        "                    pred_train_y = self.forward(train_X)\n",
        "\n",
        "                    loss = self.loss_func(pred_train_y, train_y)\n",
        "\n",
        "                    # self.zero_grad()\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                print(f\"epoch={epoch+1}: loss={loss}\")\n",
        "                # if loss < min_loss:\n",
        "                #     self.save()\n",
        "                #     min_loss = loss\n",
        "        else:\n",
        "            for epoch in range(self.epochs):\n",
        "                for batch in train_loader:\n",
        "                    train_X, train_y = batch\n",
        "                    pred_train_y = self.forward(train_X)\n",
        "\n",
        "                    loss = self.loss_func(pred_train_y, train_y)\n",
        "\n",
        "                    # self.zero_grad()\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                print(f\"epoch={epoch+1}: loss={loss}\")\n",
        "                # if loss < min_loss:\n",
        "                #     self.save()\n",
        "                #     min_loss = loss\n",
        "\n",
        "        self.save()\n",
        "\n",
        "    def test(self, test_loader):\n",
        "        self.load()\n",
        "\n",
        "        total = len(test_loader.dataset)\n",
        "        correct = 0\n",
        "\n",
        "        for batch in test_loader:\n",
        "            test_X, test_y = batch\n",
        "            pred_test_y = self.forward(test_X)\n",
        "\n",
        "            for each in range(len(test_y)):\n",
        "                pred = torch.argmax(pred_test_y, 1)[each]\n",
        "                true = torch.argmax(test_y, 1)[each]\n",
        "                if pred == true:\n",
        "                    correct += 1\n",
        "\n",
        "        print(f\"accuracy results: {correct}/{total} => {correct/total}\")\n",
        "        return correct / total"
      ],
      "metadata": {
        "id": "HDbHrRvdn6Uh"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "JQ7aLS8wqg_K"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(testing_data, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "Dp0TC6EFpXY7"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CHANNELS = 1"
      ],
      "metadata": {
        "id": "BmFBWJoTpaRY"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ViT.fit(train_dataloader, Adam, epochs=1, learning_rate=0.01, tqdm_show=True)"
      ],
      "metadata": {
        "id": "V0i9WbNvpf_a"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ViT.test(train_dataloader)"
      ],
      "metadata": {
        "id": "1zs39xNrpj6L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}