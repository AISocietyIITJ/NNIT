{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa86f70-3928-4446-85f5-ba21eac10be8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43094a6e-89fb-4788-8f50-ca643962a879",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb0250a-bcf2-4cf5-bb0a-29184c6ce52f",
   "metadata": {},
   "source": [
    "## Compiling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b2524c81-fb04-489f-8fcb-0dd9584b746f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a8e9af2c-0b9e-4cef-9682-9611faca037c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_dataset_path = '../ocr_dataset/new_data/training_data/'\n",
    "testing_dataset_path = '../ocr_dataset/new_data/testing_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b7b74e6c-14c9-4d22-a8a6-a5e986228330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "characters = os.listdir(training_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1003c1e5-8fbb-49dd-a3a9-37ce1ebe0950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels_dict = dict()\n",
    "for each in range(len(characters)):\n",
    "    labels_dict[characters[each]] = each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e3383f59-c5a1-425d-9e60-dbca8c9a873a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(characters)\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51f96156-742b-4b39-93c1-904eadfeb5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_compiler(dataset_path):\n",
    "    dataset = []\n",
    "\n",
    "    for character in characters:\n",
    "        character_path = os.path.join(dataset_path, character)\n",
    "        # print(character_path)\n",
    "\n",
    "        for character_image in os.listdir(character_path):\n",
    "            image_path = os.path.join(character_path, character_image)\n",
    "            # print(image_path)\n",
    "\n",
    "            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            # resizing the image\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            # plt.imshow(img)\n",
    "            # converting to tensor\n",
    "            img = torch.tensor(img)\n",
    "            \n",
    "            # reshaping the tensor\n",
    "            img = img.view(1, IMG_SIZE, IMG_SIZE)\n",
    "            # scaling the data\n",
    "            img = (img/255.0)\n",
    "\n",
    "            # print(img.shape)\n",
    "            dataset.append([img, np.eye(num_labels)[labels_dict[character]]])\n",
    "            \n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "58793e41-ce4e-43e2-a73a-e504b6fee5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20628"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataset = data_compiler(training_dataset_path)\n",
    "len(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "05f9c3fb-2854-456e-a6bc-5daecedaf192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1008"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_dataset = data_compiler(testing_dataset_path)\n",
    "len(testing_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e6fb87-1500-4d52-b7f3-a8a47f539b87",
   "metadata": {},
   "source": [
    "## Vision Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc161fff-5eec-416a-9b9b-c56a82ee4bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(images, patch_size):\n",
    "    '''\n",
    "    images shape: (batch_size, num_channels=1, img_size, img_size)\n",
    "    number of patches = img_size*img_size // (patch_size*patch_size)\n",
    "    output => patches shape: (batch_size, num_patches, patch_size*patch_size)\n",
    "    '''\n",
    "    batch_size, num_channels, img_size, img_size = images.shape\n",
    "    num_patches = img_size*img_size*num_channels // (patch_size*patch_size)\n",
    "    sqrt_num_patches = img_size // patch_size\n",
    "    \n",
    "    output = torch.zeros(batch_size, num_patches, patch_size*patch_size)\n",
    "    index = 0\n",
    "    for image in images[:]:\n",
    "        for c in range(num_channels):\n",
    "            for h in range(sqrt_num_patches):\n",
    "                for w in range(sqrt_num_patches):\n",
    "                    patch = image[c, h*patch_size:(h+1)*patch_size, w*patch_size:(w+1)*patch_size].flatten()\n",
    "                    output[index][c*num_patches + h*sqrt_num_patches+w] = patch\n",
    "        index+=1\n",
    "    \n",
    "    # print(output.shape)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76fe7150-c80f-4d66-8dd7-5592fae30149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_positional_embedding(sequence_length, embed_size):\n",
    "    '''\n",
    "    sequence length in our case: no. of patches + 1 (due to cls token)\n",
    "    '''\n",
    "    output = torch.zeros((sequence_length, embed_size))\n",
    "    \n",
    "    for i in range(sequence_length):\n",
    "        for j in range(embed_size):\n",
    "            if j%2 == 0:\n",
    "                output[i][j] = np.sin( i / (10000**(j/embed_size)) )\n",
    "            else:\n",
    "                output[i][j] = np.cos( i / (10000**((j-1)/embed_size)) )\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "393dafeb-d34e-4569-ab10-e1c80351da0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_shape, patch_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        batch_size, num_channels, img_size, img_size = img_shape\n",
    "        num_patches = img_size*img_size*num_channels // (patch_size*patch_size)\n",
    "    \n",
    "        # linear mapping: num_channels * patch_size * patch_size => embed_size\n",
    "        input_size = num_channels*patch_size*patch_size\n",
    "        \n",
    "        self.linear_mapping = nn.Linear(input_size, embed_size)\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.rand(1, embed_size))\n",
    "        \n",
    "        self.positional_embedding = nn.Parameter(\n",
    "            generate_positional_embedding(num_patches+1, embed_size)\n",
    "        )\n",
    "        self.positional_embedding.requires_grad = False\n",
    "        \n",
    "        '''\n",
    "        In Vision Transformers, the \"CLS token\" is required to provide\n",
    "        a global representation of the image for classification tasks,\n",
    "        allowing the model to leverage the power of the Transformer architecture\n",
    "        for image analysis and recognition.\n",
    "        '''\n",
    "        \n",
    "    def forward(self, images):\n",
    "        '''\n",
    "        input: torch tensor of image batch\n",
    "        '''\n",
    "        batch_size, num_channels, img_size, img_size = images.shape\n",
    "        patches = patchify(images, self.patch_size)\n",
    "        embeddings = self.linear_mapping(patches)    # tokens\n",
    "        \n",
    "        # adding cls tokens to tokens\n",
    "        tokens = torch.stack([torch.vstack([self.cls_token, embeddings[each]]) for each in range(len(embeddings))])\n",
    "        \n",
    "        # adding positional embedding\n",
    "        positional_embedding = self.positional_embedding.repeat(batch_size, 1, 1) \n",
    "        out = tokens + positional_embedding\n",
    "        \n",
    "        # print(f'patch embedding output shape: {out.shape}')\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9bb0763-e05a-48b9-92a0-2ac586b9043a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads\n",
    "        \n",
    "        assert self.head_dim*num_heads == embed_size, 'embedding size should be divisible by the number of heads'\n",
    "        \n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "\n",
    "    def forward(self, values, keys, queries, mask=None):\n",
    "        batch_size = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "        \n",
    "        # multi-head splitting\n",
    "        values = values.reshape(batch_size, value_len, self.num_heads, self.head_dim)\n",
    "        keys = keys.reshape(batch_size, key_len, self.num_heads, self.head_dim)\n",
    "        queries = queries.reshape(batch_size, query_len, self.num_heads, self.head_dim)\n",
    "        \n",
    "        '''\n",
    "        queries and keys multiplied when finding attention\n",
    "        n -> batch size\n",
    "        h -> no. of heads\n",
    "        q,k -> sequence length for queries and keys\n",
    "        '''\n",
    "        energy = torch.einsum('nqhd, nkhd -> nhqk', [queries, keys])\n",
    "        \n",
    "        if mask is not None:\n",
    "            # keeping a small value to be replaced by zero\n",
    "            energy = energy.masked_fill(mask == 0, float('-1e20'))\n",
    "        \n",
    "        # normalizing\n",
    "        attention = torch.softmax(energy/(self.embed_size**0.5), dim=-1)\n",
    "        \n",
    "        '''\n",
    "        attention shape: (batch size, no. of heads, query sequence length, key sequence length)\n",
    "        values shape: (batch size, value sequence length, no. of heads, head dimension)\n",
    "        output shape required: (batch size, query sequence length, no. of heads, head dimension)\n",
    "        '''\n",
    "        \n",
    "        out = torch.einsum('nhql,nlhd -> nqhd', [attention, values])\n",
    "        '''\n",
    "        n -> batch size\n",
    "        h -> no. of heads\n",
    "        q -> sequence length for queries\n",
    "        l -> sequence length for keys or values\n",
    "        d -> head dimension\n",
    "        '''\n",
    "        \n",
    "        # flattening the last two dimenstions\n",
    "        out = out.reshape(batch_size, query_len, self.num_heads*self.head_dim)\n",
    "        \n",
    "        out = self.fc_out(out)\n",
    "        \n",
    "        # print(f'attention output shape: {out.shape}')\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd143ad1-69dc-4cd9-8aa0-ae44fa373f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, embed_size, expansion=4, drop_prob=0):\n",
    "        super().__init__(\n",
    "            nn.Linear(embed_size, embed_size*expansion),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_prob),\n",
    "            nn.Linear(embed_size*expansion, embed_size)        \n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa327a9a-3180-4963-bc02-d5afe62b46ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads, drop_prob=0, forward_expansion=4, forward_drop_prob=0):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.attention = Attention(embed_size, num_heads)\n",
    "        self.dropout1 = nn.Dropout(drop_prob)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feedforward = FeedForwardBlock(embed_size, expansion=forward_expansion, drop_prob=forward_drop_prob)\n",
    "        self.dropout2 = nn.Dropout(drop_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # making copy of the tensor for residual connection\n",
    "        x_copy = x.detach().clone()\n",
    "        x_copy = self.norm1(x_copy)\n",
    "        x_copy = self.attention(x_copy, x_copy, x_copy)\n",
    "        x_copy = self.dropout1(x_copy)\n",
    "        x = x + x_copy\n",
    "        \n",
    "        # making copy of the tensor for residual connection\n",
    "        x_copy = x.detach().clone()\n",
    "        x_copy = self.norm2(x_copy)\n",
    "        x_copy = self.feedforward(x_copy)\n",
    "        x_copy = self.dropout2(x_copy)\n",
    "        x = x + x_copy\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33c3301e-9ae4-4f47-ad05-3fd4fee21990",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_shape, patch_size, num_heads, embed_size, encoder_depth, num_classes):\n",
    "        '''\n",
    "        img_shape: shape of the batch of images passed\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            img_shape=img_shape,\n",
    "            patch_size=patch_size,\n",
    "            embed_size=embed_size\n",
    "        )\n",
    "        \n",
    "        self.encoder = nn.ModuleList(\n",
    "            [\n",
    "                EncoderBlock(embed_size, num_heads) for _ in range(encoder_depth)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # classification head\n",
    "        self.classification_mlp = nn.Sequential(\n",
    "            nn.Linear(embed_size, num_classes),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, images):\n",
    "        out = self.patch_embedding(images)\n",
    "        \n",
    "        for encoderblocks in self.encoder:\n",
    "            out = encoderblocks(out)\n",
    "            \n",
    "        # getting the classification tokens\n",
    "        out = out[:, 0]\n",
    "        out = self.classification_mlp(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def save(self, path='best_model.pth'):\n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path='best_model.pth'):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.eval()\n",
    "        \n",
    "    def fit(self, train_loader, optimizer=Adam, loss_function=CrossEntropyLoss(), learning_rate=0.001, epochs=5, tqdm_show=False):\n",
    "        min_loss = float('inf')\n",
    "        self.optimizer = optimizer(self.parameters(), lr=learning_rate)\n",
    "        self.loss_func = loss_function\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        if tqdm_show:\n",
    "            for epoch in tqdm(range(self.epochs)):\n",
    "                for batch in tqdm(train_loader):\n",
    "                    train_X, train_y = batch\n",
    "                    pred_train_y = self.forward(train_X)\n",
    "\n",
    "                    loss = self.loss_func(pred_train_y, train_y)\n",
    "\n",
    "                    # self.zero_grad()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                print(f'epoch={epoch+1}: loss={loss}')\n",
    "                if loss < min_loss:\n",
    "                    self.save()\n",
    "                    min_loss = loss\n",
    "        else:\n",
    "            for epoch in range(self.epochs):\n",
    "                for batch in train_loader:\n",
    "                    train_X, train_y = batch\n",
    "                    pred_train_y = self.forward(train_X)\n",
    "\n",
    "                    loss = self.loss_func(pred_train_y, train_y)\n",
    "\n",
    "                    # self.zero_grad()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                print(f'epoch={epoch+1}: loss={loss}')\n",
    "                if loss < min_loss:\n",
    "                    self.save()\n",
    "                    min_loss = loss\n",
    "            \n",
    "        self.save()\n",
    "        \n",
    "    def test(self, test_loader):\n",
    "        self.load()\n",
    "        \n",
    "        total = len(test_loader.dataset)\n",
    "        correct = 0\n",
    "        \n",
    "        for batch in test_loader:\n",
    "            test_X, test_y = batch\n",
    "            pred_test_y = self.forward(test_X)\n",
    "\n",
    "            for each in range(len(test_y)):\n",
    "                pred = torch.argmax(pred_test_y, 1)[each]\n",
    "                true = torch.argmax(test_y, 1)[each]\n",
    "                if pred == true:\n",
    "                    correct += 1\n",
    "                    \n",
    "        print(f'accuracy results: {correct}/{total} => {correct/total}')\n",
    "        return correct/total\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9249578-17f9-4c87-8cd7-c1b5b889d072",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9664c7e-4dcb-4f35-96a1-7b4c8b932acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c17c7fd6-dba8-4601-92ac-99af16dc02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(testing_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e8bf0e6-9beb-46f0-84fd-607935e1ce7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "81576719-b464-4f7b-be7c-682d632b730c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vit = VisionTransformer(\n",
    "    img_shape=(BATCH_SIZE, NUM_CHANNELS, IMG_SIZE, IMG_SIZE),\n",
    "    patch_size = 8,\n",
    "    num_heads = 4,\n",
    "    embed_size = 128,\n",
    "    encoder_depth=6,\n",
    "    num_classes=num_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b7680c12-68f8-4a5f-a24e-80ba63917de9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─PatchEmbedding: 1-1                    --\n",
      "|    └─Linear: 2-1                       8,320\n",
      "├─ModuleList: 1-2                        --\n",
      "|    └─EncoderBlock: 2-2                 --\n",
      "|    |    └─LayerNorm: 3-1               256\n",
      "|    |    └─Attention: 3-2               19,584\n",
      "|    |    └─Dropout: 3-3                 --\n",
      "|    |    └─LayerNorm: 3-4               256\n",
      "|    |    └─FeedForwardBlock: 3-5        131,712\n",
      "|    |    └─Dropout: 3-6                 --\n",
      "|    └─EncoderBlock: 2-3                 --\n",
      "|    |    └─LayerNorm: 3-7               256\n",
      "|    |    └─Attention: 3-8               19,584\n",
      "|    |    └─Dropout: 3-9                 --\n",
      "|    |    └─LayerNorm: 3-10              256\n",
      "|    |    └─FeedForwardBlock: 3-11       131,712\n",
      "|    |    └─Dropout: 3-12                --\n",
      "|    └─EncoderBlock: 2-4                 --\n",
      "|    |    └─LayerNorm: 3-13              256\n",
      "|    |    └─Attention: 3-14              19,584\n",
      "|    |    └─Dropout: 3-15                --\n",
      "|    |    └─LayerNorm: 3-16              256\n",
      "|    |    └─FeedForwardBlock: 3-17       131,712\n",
      "|    |    └─Dropout: 3-18                --\n",
      "|    └─EncoderBlock: 2-5                 --\n",
      "|    |    └─LayerNorm: 3-19              256\n",
      "|    |    └─Attention: 3-20              19,584\n",
      "|    |    └─Dropout: 3-21                --\n",
      "|    |    └─LayerNorm: 3-22              256\n",
      "|    |    └─FeedForwardBlock: 3-23       131,712\n",
      "|    |    └─Dropout: 3-24                --\n",
      "|    └─EncoderBlock: 2-6                 --\n",
      "|    |    └─LayerNorm: 3-25              256\n",
      "|    |    └─Attention: 3-26              19,584\n",
      "|    |    └─Dropout: 3-27                --\n",
      "|    |    └─LayerNorm: 3-28              256\n",
      "|    |    └─FeedForwardBlock: 3-29       131,712\n",
      "|    |    └─Dropout: 3-30                --\n",
      "|    └─EncoderBlock: 2-7                 --\n",
      "|    |    └─LayerNorm: 3-31              256\n",
      "|    |    └─Attention: 3-32              19,584\n",
      "|    |    └─Dropout: 3-33                --\n",
      "|    |    └─LayerNorm: 3-34              256\n",
      "|    |    └─FeedForwardBlock: 3-35       131,712\n",
      "|    |    └─Dropout: 3-36                --\n",
      "├─Sequential: 1-3                        --\n",
      "|    └─Linear: 2-8                       4,644\n",
      "|    └─Softmax: 2-9                      --\n",
      "=================================================================\n",
      "Total params: 923,812\n",
      "Trainable params: 923,812\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "├─PatchEmbedding: 1-1                    --\n",
       "|    └─Linear: 2-1                       8,320\n",
       "├─ModuleList: 1-2                        --\n",
       "|    └─EncoderBlock: 2-2                 --\n",
       "|    |    └─LayerNorm: 3-1               256\n",
       "|    |    └─Attention: 3-2               19,584\n",
       "|    |    └─Dropout: 3-3                 --\n",
       "|    |    └─LayerNorm: 3-4               256\n",
       "|    |    └─FeedForwardBlock: 3-5        131,712\n",
       "|    |    └─Dropout: 3-6                 --\n",
       "|    └─EncoderBlock: 2-3                 --\n",
       "|    |    └─LayerNorm: 3-7               256\n",
       "|    |    └─Attention: 3-8               19,584\n",
       "|    |    └─Dropout: 3-9                 --\n",
       "|    |    └─LayerNorm: 3-10              256\n",
       "|    |    └─FeedForwardBlock: 3-11       131,712\n",
       "|    |    └─Dropout: 3-12                --\n",
       "|    └─EncoderBlock: 2-4                 --\n",
       "|    |    └─LayerNorm: 3-13              256\n",
       "|    |    └─Attention: 3-14              19,584\n",
       "|    |    └─Dropout: 3-15                --\n",
       "|    |    └─LayerNorm: 3-16              256\n",
       "|    |    └─FeedForwardBlock: 3-17       131,712\n",
       "|    |    └─Dropout: 3-18                --\n",
       "|    └─EncoderBlock: 2-5                 --\n",
       "|    |    └─LayerNorm: 3-19              256\n",
       "|    |    └─Attention: 3-20              19,584\n",
       "|    |    └─Dropout: 3-21                --\n",
       "|    |    └─LayerNorm: 3-22              256\n",
       "|    |    └─FeedForwardBlock: 3-23       131,712\n",
       "|    |    └─Dropout: 3-24                --\n",
       "|    └─EncoderBlock: 2-6                 --\n",
       "|    |    └─LayerNorm: 3-25              256\n",
       "|    |    └─Attention: 3-26              19,584\n",
       "|    |    └─Dropout: 3-27                --\n",
       "|    |    └─LayerNorm: 3-28              256\n",
       "|    |    └─FeedForwardBlock: 3-29       131,712\n",
       "|    |    └─Dropout: 3-30                --\n",
       "|    └─EncoderBlock: 2-7                 --\n",
       "|    |    └─LayerNorm: 3-31              256\n",
       "|    |    └─Attention: 3-32              19,584\n",
       "|    |    └─Dropout: 3-33                --\n",
       "|    |    └─LayerNorm: 3-34              256\n",
       "|    |    └─FeedForwardBlock: 3-35       131,712\n",
       "|    |    └─Dropout: 3-36                --\n",
       "├─Sequential: 1-3                        --\n",
       "|    └─Linear: 2-8                       4,644\n",
       "|    └─Softmax: 2-9                      --\n",
       "=================================================================\n",
       "Total params: 923,812\n",
       "Trainable params: 923,812\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1eb52c-aee1-4c20-8e30-92bfdd1d100e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vit.fit(train_dataloader, epochs=20, learning_rate=0.0005, tqdm_show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a74503-bf19-42f5-8a51-f0ae59d0ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit.test(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626d227d-2660-41bd-a90d-c9bb7a0899b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vit.test(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e8823f-7f6c-4fe2-ab40-de1a81af8023",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1ed44f-2943-4f33-aef5-b40d3b890121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe96897-a381-4f61-b664-60ce4c97fae0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
